{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ranking policy training"
   ],
   "id": "1e3d66a85d15b9ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "The tree nodes in MCTS are expanded by an expansion function approximated by a policy graph neural network. The policy network is composed of two parts: molecular representation and reaction rule prediction parts. In the representation part, the molecular graph is converted to a single vector by graph convolutional layers. The training set structure and the prediction part architecture depend on the type of policy network, particularly the ranking or filtering policy network.\n",
    "\n",
    "**Ranking policy network**. The training dataset for ranking policy network consists of pairs of reactions and corresponding reaction rules extracted from it. The products of the reaction are transformed to the CGR encoded as a molecular graph with the one-hot encoded label vector where the positive label corresponds to the reaction rule. The prediction part is terminated with the softmax function generating the “probability of successful application” of each reaction rule to a given input molecular graph, which can be used for the reaction rules “ranking”.\n",
    "\n",
    "**Filtering policy network**. The training dataset for the filtering policy is formed by the application of all reaction rules to the training molecules. The labels vector is filled with positive labels in positions corresponding to the successfully applied reaction rules. The prediction part of the filtering policy is formed from two linear layers with a sigmoid function that assigns the probabilities for the “regular”, as well as “priority” reaction rules (cyclization reaction rules). These two vectors are then combined with a coefficient α ranging from 0 to 1. This approach ensures that the priority reaction rules receive the highest score, followed by other regular reaction rules.\n",
    " \n",
    "<div class=\"alert alert-info\">\n",
    "**Note**  \n",
    "    \n",
    "The filtering policy network requires much more computational resources for the generating of the training dataset than the ranking policy but can be used with any set of reaction rules because the original reaction dataset is not needed. This allows for the portability of reaction rules extracted with another software from any source of reaction data.\n",
    "</div> \n",
    "\n",
    "The tree nodes in MCTS are expanded by an expansion function approximated by a policy graph neural network. The policy network is composed of two parts: molecular representation and reaction rule prediction parts. In the representation part, the molecular graph is converted to a single vector by graph convolutional layers. The training set structure and the prediction part architecture depend on the type of policy network, particularly the ranking or filtering policy network. The training dataset for ranking policy network consists of pairs of reactions and corresponding reaction rules extracted from it. The products of the reaction are transformed to the CGR encoded as a molecular graph with the one-hot encoded label vector where the positive label corresponds to the reaction rule. The prediction part is terminated with the softmax function generating the “probability of successful application” of each reaction rule to a given input molecular graph, which can be used for the reaction rules “ranking”.\n",
    "All the reaction rules predicted by the ranking or policy neural network are sorted by predicted reaction rule probability and the first N (usually N = 50) of reaction rules are selected to be applied to the current precursor in the expansion step.\n",
    "\n",
    "Frst, we define the training configuration using the `PolicyNetworkConfig` class. This configuration includes various hyperparameters for the neural network:"
   ],
   "id": "2e72baa24ee119f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 19,
   "source": [
    "from synplan.utils.config import PolicyNetworkConfig\n",
    "from synplan.ml.training.supervised import create_policy_dataset, run_policy_training"
   ],
   "id": "e847c9027e3141f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuration\n",
    "**Configuration parameters**:\n",
    "\n",
    "| Parameter        | Default | Description                                                     |\n",
    "|------------------|---------|-----------------------------------------------------------------|\n",
    "| vector_dim       | 512     | The dimension of the hidden layers                              |\n",
    "| num_conv_layers  | 5       | The number of convolutional layers                              |\n",
    "| learning_rate    | 0.0005  | The learning rate                                               |\n",
    "| dropout          | 0.4     | The dropout value                                               |\n",
    "| num_epoch        | 100     | The number of training epochs                                   |\n",
    "| batch_size       | 1000    | The size of the training batch of input molecular graphs        |\n",
    "\n",
    "The ranking or filtering policy network architecture and training hyperparameters can be adjusted in the training configuration yaml file below."
   ],
   "id": "6ca9e6ee052a8553"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 20,
   "source": [
    "training_config = PolicyNetworkConfig(\n",
    "    policy_type=\"ranking\",  # the type of policy network\n",
    "    num_conv_layers=5,  # the number of graph convolutional layers in the network\n",
    "    vector_dim=512,  # the dimensionality of the final embedding vector\n",
    "    learning_rate=0.0008,  # the learning rate for the training process\n",
    "    dropout=0.4,  # the dropout rate\n",
    "    num_epoch=100,  # the number of epochs for training\n",
    "    batch_size=100,\n",
    ")  # the size of training batch of input data"
   ],
   "id": "9457427e36dd79d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Creating the training set\n",
    "\n",
    "Next, we create the policy dataset using the `create_policy_dataset` function. This involves specifying paths to the reaction rules and the reaction data:"
   ],
   "id": "94f5db29397d4d95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of reactions processed: 69445 [11:33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 52509, validation set size: 13128\n"
     ]
    }
   ],
   "execution_count": 21,
   "source": [
    "ranking_policy_network_folder = root_folder.joinpath(\"ranking_policy_network\")\n",
    "ranking_policy_dataset_path = ranking_policy_network_folder.joinpath(\"ranking_policy_dataset.pt\")\n",
    "\n",
    "datamodule = create_policy_dataset(\n",
    "    dataset_type=\"ranking\",\n",
    "    reaction_rules_path=reaction_rules_path,\n",
    "    molecules_or_reactions_path=filtered_data_path,\n",
    "    output_path=ranking_policy_dataset_path,\n",
    "    batch_size=training_config.batch_size,\n",
    "    num_cpus=4,\n",
    ")"
   ],
   "id": "2905b131ed0268c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Run the policy network training\n",
    "\n",
    "Finally, we train the policy network using the `run_policy_training` function. This step involves feeding the dataset and the training configuration into the network:"
   ],
   "id": "6f9e9e8cd9423f30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "Epoch 00012: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch 00020: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch 00024: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Epoch 00028: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch 00032: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Epoch 00048: reducing learning rate of group 0 to 8.5899e-05.\n",
      "Epoch 00052: reducing learning rate of group 0 to 6.8719e-05.\n",
      "Epoch 00056: reducing learning rate of group 0 to 5.4976e-05.\n",
      "Epoch 00060: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Policy network balanced accuracy: 0.985\n"
     ]
    }
   ],
   "execution_count": 22,
   "source": [
    "run_policy_training(\n",
    "    datamodule,  # the prepared data module for training\n",
    "    config=training_config,  # the training configuration\n",
    "    results_path=ranking_policy_network_folder,\n",
    ")  # path to save the training results"
   ],
   "id": "d59c420b56fb3e00"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
