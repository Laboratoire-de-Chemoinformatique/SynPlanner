{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e3d66a85d15b9ca",
   "metadata": {},
   "source": [
    "# Step 3: Policy Network Training\n",
    "\n",
    "This tutorial demonstrates how to train ranking and filtering policy netwirk in ``SynPlanner``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43180f-3215-444d-97f7-c19292ba9591",
   "metadata": {},
   "source": [
    "## Basic recommendations\n",
    "\n",
    "**1. Prefer ranking policy network over filtering policy network**\n",
    "\n",
    "The filtering policy network in its current implementation requires a lot of computational resources and its training is practically feasible with many CPUs and several dozen GB of RAM in case of large training sets. The bottleneck of the current implementation is the preparation of the training dataset, particularly the generation of binary vectors if successfully applied reaction rules to each training molecule. Thus, with limited computational\n",
    "resources, it is recommended to use a ranking policy network.\n",
    "\n",
    "**2. Use a filtering policy network for the portability of reaction rules between different tools**\n",
    "\n",
    "Filtering policy networks can be trained with any set of reaction rules, including those generated with other software because filtering network training does not depend on the original reaction dataset from which the reaction rules were extracted. In this case, the filtering policy network can be used for comparison of reaction rules extracted with different software/tools.\n",
    "\n",
    "**3. Reduce the size of the training molecules for filtering policy network**\n",
    "\n",
    "The problem of computational resources for filtering policy networks can be partially solved by a drastic reduction of the training set of molecules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39ecf78-7103-4fa1-8ab0-88c995d40350",
   "metadata": {},
   "source": [
    "## 1. Set up input and output data locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944dc45c-c5b3-4255-82de-667edf364b3e",
   "metadata": {},
   "source": [
    "The ``SynPlanner`` input data will be downloaded from the ``HuggingFace`` repository to the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928513a8-df22-4927-937a-230799d49d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d2f5655d3b4dc4a0f5357c3074b5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 25 files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from synplan.utils.loading import download_all_data\n",
    "\n",
    "# download SynPlanner data\n",
    "data_folder = Path(\"synplan_data\").resolve()\n",
    "download_all_data(save_to=data_folder)\n",
    "\n",
    "# results folder\n",
    "results_folder = Path(\"tutorial_results\").resolve()\n",
    "results_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# input data\n",
    "# use default filtered data from tutorial folder or replace with custom data prepared with data curation tutorial\n",
    "# be sure that you use the same reaction dataset from which the reaction rules were extracted \n",
    "\n",
    "reaction_rules_path = results_folder.joinpath(\"uspto_reaction_rules.pickle\") # needed for both ranking and filtering policy network training\n",
    "\n",
    "filtered_data_path = results_folder.joinpath(\"uspto_filtered.smi\") # needed for ranking policy network training\n",
    "molecules_data_path = results_folder.joinpath(\"\") # needed for filtering policy network training\n",
    "\n",
    "# output data\n",
    "ranking_policy_network_folder = results_folder.joinpath(\"ranking_policy_network\")\n",
    "filtering_policy_network_folder = results_folder.joinpath(\"filtering_policy_network\")\n",
    "\n",
    "# output data\n",
    "ranking_policy_dataset_path = ranking_policy_network_folder.joinpath(\"ranking_policy_dataset.pt\") # the generated training set for ranking network\n",
    "filtering_policy_dataset_path = filtering_policy_network_folder.joinpath(\"filtering_policy_dataset.pt\") # the generated training set for ranking network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7178d-ea22-4fcc-a39d-00ae95c24cdf",
   "metadata": {},
   "source": [
    "## 2. Ranking policy training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ec9665-b316-41d3-9c85-45f9e5016f93",
   "metadata": {},
   "source": [
    "### Ranking network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d0ad0a2-4fad-47f5-ab5c-6f5b38343e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synplan.utils.config import PolicyNetworkConfig\n",
    "from synplan.ml.training.supervised import create_policy_dataset, run_policy_training\n",
    "\n",
    "training_config = PolicyNetworkConfig(\n",
    "    policy_type=\"ranking\",  # the type of policy network\n",
    "    num_conv_layers=5,  # the number of graph convolutional layers in the network\n",
    "    vector_dim=512,  # the dimensionality of the final embedding vector\n",
    "    learning_rate=0.0008,  # the learning rate for the training process\n",
    "    dropout=0.4,  # the dropout rate\n",
    "    num_epoch=100,  # the number of epochs for training\n",
    "    batch_size=100,\n",
    ")  # the size of training batch of input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f5db29397d4d95",
   "metadata": {},
   "source": [
    "### Creating ranking network training set\n",
    "\n",
    "Next, we create the policy dataset using the `create_policy_dataset` function. This involves specifying paths to the reaction rules and the reaction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2905b131ed0268c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of reactions processed: 69445 [09:36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 52512, validation set size: 13128\n"
     ]
    }
   ],
   "source": [
    "datamodule = create_policy_dataset(\n",
    "    dataset_type=\"ranking\",\n",
    "    reaction_rules_path=reaction_rules_path,\n",
    "    molecules_or_reactions_path=filtered_data_path,\n",
    "    output_path=ranking_policy_dataset_path,\n",
    "    batch_size=training_config.batch_size,\n",
    "    num_cpus=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e9e8cd9423f30",
   "metadata": {},
   "source": [
    "### Running ranking policy network training\n",
    "\n",
    "Finally, we train the policy network using the `run_policy_training` function. This step involves feeding the dataset and the training configuration into the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d59c420b56fb3e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy network balanced accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "run_policy_training(\n",
    "    datamodule,  # the prepared data module for training\n",
    "    config=training_config,  # the training configuration\n",
    "    results_path=ranking_policy_network_folder,\n",
    ")  # path to save the training results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8006e0-3dc7-4763-a8ec-0ab89caaacdd",
   "metadata": {},
   "source": [
    "## 3. Filtering policy training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06f811-c043-459d-bd9f-b7f444c9eb5f",
   "metadata": {},
   "source": [
    "### Filtering network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c93e76e-d5a6-4443-a2f5-445b1c6dd79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synplan.utils.config import PolicyNetworkConfig\n",
    "from synplan.ml.training.supervised import create_policy_dataset, run_policy_training\n",
    "\n",
    "training_config = PolicyNetworkConfig(\n",
    "    policy_type=\"filtering\",  # the type of policy network\n",
    "    num_conv_layers=5,  # the number of graph convolutional layers in the network\n",
    "    vector_dim=512,  # the dimensionality of the final embedding vector\n",
    "    learning_rate=0.0008,  # the learning rate for the training process\n",
    "    dropout=0.4,  # the dropout rate\n",
    "    num_epoch=100,  # the number of epochs for training\n",
    "    batch_size=100,\n",
    ")  # the size of training batch of input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feee9a4-19ee-470a-b535-41478e0624e1",
   "metadata": {},
   "source": [
    "### Creating filtering network training set\n",
    "\n",
    "Next, we create the policy dataset using the `create_policy_dataset` function. This involves specifying paths to the reaction rules and the molecules dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bbf54db-112e-4b1c-b512-5f48f04ffd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = create_policy_dataset(\n",
    "    dataset_type=\"filtering\",\n",
    "    reaction_rules_path=reaction_rules_path,\n",
    "    molecules_or_reactions_path=filtered_data_path,\n",
    "    output_path=filtering_policy_dataset_path,\n",
    "    batch_size=training_config.batch_size,\n",
    "    num_cpus=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796eb30-98cd-4208-b04c-33f9a1bb3b20",
   "metadata": {},
   "source": [
    "### Running filtering policy network training\n",
    "\n",
    "Finally, we train the policy network using the `run_policy_training` function. This step involves feeding the dataset and the training configuration into the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d42ff2c-96ca-49f3-8cb2-54ced3063d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_policy_training(\n",
    "    datamodule,  # the prepared data module for training\n",
    "    config=training_config,  # the training configuration\n",
    "    results_path=filtering_policy_network_folder,\n",
    ")  # path to save the training results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8fca9f-c0e6-4aac-a12d-32a77ce8ad69",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "If the tutorial is executed successfully, you will get in the results folder three reaction data files (from reaction curation tutorial), corresponding extracted reaction rules (from reaction rules extraction tutorial) and trained ranking and filtering policy network:\n",
    "- original reaction data\n",
    "- standardized reaction data\n",
    "- filtered reaction data\n",
    "- extracted reaction rules\n",
    "- ranking policy network folder (the training set and trained network)\n",
    "- filtering policy network folder (the training set and trained network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c69c834b-b821-495a-acd9-6beb8a5a5cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home1/dima/synplanner/docs_tutorials/tutorial_results/uspto_standardized.smi'),\n",
       " PosixPath('/home1/dima/synplanner/docs_tutorials/tutorial_results/uspto_filtered.smi'),\n",
       " PosixPath('/home1/dima/synplanner/docs_tutorials/tutorial_results/uspto_original.smi'),\n",
       " PosixPath('/home1/dima/synplanner/docs_tutorials/tutorial_results/uspto_reaction_rules.pickle'),\n",
       " PosixPath('/home1/dima/synplanner/docs_tutorials/tutorial_results/ranking_policy_network')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Path(results_folder).iterdir(), key=os.path.getmtime, reverse=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44ba8c5a-067f-4503-b2ba-0dc504443ce8",
   "metadata": {},
   "source": [
    "# Next step: Retrosynthetic planning\n",
    "\n",
    "As soon as the policy network is trained, it can be used in retrosynthetic planning for target molecules."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synplan_test",
   "language": "python",
   "name": "synplan_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
